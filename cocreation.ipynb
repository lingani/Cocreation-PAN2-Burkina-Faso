{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lingani\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "## 1 - Consultations en ligne\n",
    "########################################################\n",
    "%matplotlib inline\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des données de google spreadsheet\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "\n",
    "#fonction d'importation des données\n",
    "def read_data(data_url, google_credentials):\n",
    "    # use creds to create a client to interact with the Google Drive API\n",
    "    scope = ['https://spreadsheets.google.com/feeds']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(google_credentials, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    # Find a workbook by name and open the first sheet\n",
    "    # Make sure you use the right url here.\n",
    "    sheet = client.open_by_url(data_url)\n",
    "    worksheet = sheet.get_worksheet(0)\n",
    "    # Extract and print all of the values\n",
    "    list_of_hashes = worksheet.get_all_records()\n",
    "    # print(list_of_hashes)\n",
    "    return pd.DataFrame(list_of_hashes)\n",
    "\n",
    "data_url = 'https://docs.google.com/spreadsheets/d/1TvjqPwKQJZhqT8TxD1iCO_-MvqeQ2eq3mECFQhfh6_c/edit?usp=sharing'\n",
    "google_credentials = 'cocreation-f23bef2d618c.json'\n",
    "propositions = read_data(data_url, google_credentials)\n",
    "# propositions.head(5) # de-commentez cette ligne pour afficher les données et en avoir un apperçu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, nous allons faire quelques petites transformations pour plus de convenance\n",
    "# propositions.columns # de-commentez cette ligne pour afficher les noms des colonnes\n",
    "# Renommez les noms des colonnes pour des questions pratiques de manipulation des données\n",
    "propositions.rename(columns={u\"Dans quelle région residez-vous?\": 'region', \n",
    "                   u'Domaines concernés par votre proposition': 'domaine',\n",
    "                  u'Nom': 'nom',\n",
    "                  u'Prénom': 'prenom',\n",
    "                  u'E-mail': 'email',\n",
    "                  u'Précisez la Ville ou la Province': 'province/ville',\n",
    "                  u'Quel est votre proposition de solution': 'solution',\n",
    "                  u'Quel problème souhaitez-vous résoudre?': 'probleme',\n",
    "                  u'Sexe':'sexe',\n",
    "                  u'Si vous avez sélectionner “Autre“ précédemment, précisé le pays dans lequel vous résidez': 'pays',\n",
    "                  u\"Souhaitez-vous être contacter par le comité du PGO afin d'approfondir la réflexion sur la solution que vous proposez? \": 'etre_contacte',\n",
    "                  u'Timestamp': 'timestamp',\n",
    "                  u\"Tranche d'age\": 'age',\n",
    "                  u'Téléphone (WhatsApp)': u'phone(whatsApp)'}, inplace=True)\n",
    "\n",
    "# propositions.columns # de-commentez cette ligne pour afficher les nouvaux noms des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créons un corpus de texte pour les problèmes et les solutions\n",
    "texte_problemes = \" \".join(propositions[\"probleme\"])\n",
    "texte_solutions = \" \".join(propositions[\"solution\"])\n",
    "texte_domaines1 = \"\\n\".join(propositions[\"domaine\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deux petites fonctions pour le nettotyage des corpus de texte\n",
    "import nltk\n",
    "def clean_tokens(texte):\n",
    "    # transformons tous les textes en miniscule\n",
    "    texte = texte.lower()\n",
    "    texte = texte.replace(')', ' ')\n",
    "    texte = texte.replace('(', ' ')\n",
    "    texte = texte.replace(';', ' ')\n",
    "    texte = texte.replace(',', ' ')\n",
    "    texte = texte.replace('.', ' ')\n",
    "\n",
    "    tokens_texte = [t for t in texte.split()]\n",
    "    # supprimer les ponctuations et les mots courts\n",
    "    for w in tokens_texte:\n",
    "        w = w.strip('\\'\"?,.!_+=-')\n",
    "    for w in tokens_texte:\n",
    "        if len(w) < 3:\n",
    "            tokens_texte.remove(w) \n",
    "        \n",
    "    # suppression des mots de liason\n",
    "    from nltk.corpus import stopwords\n",
    "    sr = stopwords.words('french')\n",
    "    # trouver les stopwords français et le mettre dans sr\n",
    "    french = pd.read_csv(\"french.csv\")\n",
    "    sr += list(french[\"words\"])\n",
    "    sr = list(set(sr))\n",
    "    \n",
    "    clean_tokens_texte = tokens_texte[:]\n",
    "    for token in tokens_texte:\n",
    "        if token in sr:\n",
    "            clean_tokens_texte.remove(token)\n",
    "    \n",
    "    mots_courts = ['osc', 'ong', 'loi', 'iec', 'fsi', 'fds', 'eau', 'axe', 'rue', 'tic']\n",
    "    for token  in clean_tokens_texte:  \n",
    "        if (len(token)<4) and (token not in mots_courts):\n",
    "            clean_tokens_texte.remove(token)\n",
    "    \n",
    "    return clean_tokens_texte\n",
    "\n",
    "\n",
    "def visual_cleaning(texte):\n",
    "    import re\n",
    "    from textblob import TextBlob\n",
    "    texte = texte.lower()\n",
    "\n",
    "    texte = texte.replace('renforcement', 'renforcer')\n",
    "    texte = texte.replace('sensibiliser', 'sensibilisation')\n",
    "    texte = texte.replace('dans', '')\n",
    "    texte = texte.replace('-', '')\n",
    "    texte = texte.replace('/', ' ')\n",
    "    texte = texte.replace(u'\\xc3\\xa0', ' ')\n",
    "    texte = texte.replace(u'\\xe2\\x80\\x99', ' ')\n",
    "    \n",
    "    texte = re.sub(r'[0-9]+', '', texte)\n",
    "    \n",
    "    # suppression des pluriels et des mots courts    \n",
    "    text_blob_object = TextBlob(texte)\n",
    "    words = text_blob_object.words\n",
    "    \n",
    "    texte = \"\"\n",
    "    for w in words:\n",
    "        if len(w) > 4:\n",
    "            w = w.singularize()\n",
    "        texte = \" \".join([texte, w])\n",
    "\n",
    "    # harmonisation de quelques adjectifs\n",
    "    texte = texte.replace('locaux', 'local')\n",
    "    texte = texte.replace('locale ', 'local ')\n",
    "    texte = texte.replace('fond ', 'fonds ')\n",
    "    texte = texte.replace('apfr ', ' ')\n",
    "    texte = texte.replace(u'\\u2019', u'')\n",
    "    texte = texte.replace('ev ', ' ')\n",
    "    texte = texte.replace('prd ', ' ')\n",
    "    texte = texte.replace(u'acc\\xe8 ', u'acc\\xe8s ')\n",
    "    texte = texte.replace(u'pnkt', u'')\n",
    "    texte = texte.replace('renforcement', 'renforcer')\n",
    "    texte = texte.replace(u'fonci\\xe8re', u'foncier')\n",
    "    \n",
    "    return texte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lingani/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:30: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "# Nettoyons les deux corpus pour y soustraire les ponctuations et les mots de liaison\n",
    "texte_problemes = visual_cleaning(texte_problemes)\n",
    "texte_solutions = visual_cleaning(texte_solutions)\n",
    "\n",
    "cleaned_tokens_problemes = clean_tokens(texte_problemes)\n",
    "cleaned_tokens_solutions = clean_tokens(texte_solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cráetion du tableau de frequence des mots \n",
    "freq_problemes = nltk.FreqDist(cleaned_tokens_problemes)\n",
    "freq_solutions = nltk.FreqDist(cleaned_tokens_solutions)\n",
    "# d-commentez les deux ligne ci-dessous pour voir le tableau des frequences\n",
    "# for key,val in freq.items():\n",
    "#    print(key.encode('utf-8') + ':' + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les graphques de frequence des mots pour les preocupations exprimées en régions\n",
    "# freq_problemes.plot(20, cumulative=False) # uncomment to see output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les graphques de frequence des mots pour les propositions de solutions en régions\n",
    "# freq_solutions.plot(20, cumulative=False) # uncomment to see output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "## 2 - Consultations regionales\n",
    "########################################################\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "#importation des données de google spreadsheet\n",
    "data_url = 'https://docs.google.com/spreadsheets/d/1x7FFFmqHeQwEmJx1iaP1zEX_Bh3AglVZZJG9pIwWusc/edit?usp=sharing'\n",
    "google_credentials = 'cocreation-f23bef2d618c.json'\n",
    "propositions = read_data(data_url, google_credentials)\n",
    "# propositions.head(5) # de-commentez cette ligne pour afficher les données et en avoir un apperçu\n",
    "# Vous pouvez aussi utiliser la ligne ci-dessous en ayant prealablement telecharger les données en format csv sur votre disque local\n",
    "# propositions = pd.read_csv(\"consultations_regions.csv\")\n",
    "\n",
    "texte_problemes = \" \".join(propositions[\"probleme\"])\n",
    "texte_solutions = \" \".join(propositions[\"solution\"])\n",
    "texte_domaines2 = \"\\n\".join(propositions[\"domaine\"])\n",
    "\n",
    "\n",
    "\n",
    "texte_problemes = visual_cleaning(texte_problemes)\n",
    "texte_solutions = visual_cleaning(texte_solutions)\n",
    "\n",
    "cleaned_tokens_problemes = clean_tokens(texte_problemes)\n",
    "cleaned_tokens_solutions = clean_tokens(texte_solutions)\n",
    "\n",
    "#cráetion du tableau de frequence des mots \n",
    "freq_problemes = nltk.FreqDist(cleaned_tokens_problemes)\n",
    "freq_solutions = nltk.FreqDist(cleaned_tokens_solutions)\n",
    "# d-commentez les deux ligne ci-dessous pour voir le tableau des frequences\n",
    "# for key,val in freq.items():\n",
    "#    print(key.encode('utf-8') + ':' + str(val))\n",
    "\n",
    "# Afficher les graphques de frequence des mots pour les preocupations exprimées en ligne\n",
    "# freq_problemes.plot(20, cumulative=False) # uncomment to see output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les graphques de frequence des mots pour les propositions de solutions faites en ligne\n",
    "# freq_solutions.plot(20, cumulative=False) # uncomment to see output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de nuages de mots sur les consultations en régions\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "def makeImage(text):\n",
    "    alice_mask = np.array(Image.open(\"bf9.jpg\"))\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", max_words=100, mask=alice_mask)\n",
    "    # generate word cloud\n",
    "    wc.generate(text)\n",
    "\n",
    "    # show\n",
    "    plt.figure(figsize=(15,15), dpi=100)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "texte_problemes = \" \".join(cleaned_tokens_problemes)\n",
    "texte_solutions = \" \".join(cleaned_tokens_solutions)\n",
    "\n",
    "# makeImage(texte_problemes.decode(\"utf-8\")) # un-comment to see the wordcloud for problems\n",
    "makeImage(texte_solutions.decode(\"utf-8\")) # un-comment to see the wordcloud for solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "## 3 - Analyse des domaines des preocupations\n",
    "########################################################\n",
    "import re\n",
    "def clean_and_tokenize(texte, t_agg):\n",
    "    \n",
    "    # lower texte\n",
    "    texte = texte.lower()\n",
    "    \n",
    "    # each tocken on a line\n",
    "    texte = texte.encode(\"utf-8\")\n",
    "    texte = texte.replace(\",\", \"\\n\")\n",
    "    texte = texte.replace(\";\", \"\\n\")\n",
    "    texte = texte.replace(\"(\", \"\\n\")\n",
    "    texte = texte.replace(\")\", \"\")\n",
    "    texte = texte.replace(\"/\", \"\\n\")\n",
    "    texte = texte.replace(\"l'\", \"\")\n",
    "\n",
    "    # trim spaces\n",
    "    texte = re.sub('( )( )*\\n', '\\n', texte)\n",
    "    texte = re.sub('\\n( )( )*', '\\n', texte)\n",
    "    texte = re.sub('\\n(\\n)*', '\\n', texte)\n",
    "    texte = re.sub('\\nla ', '\\n', texte)\n",
    "    texte = re.sub('\\nle ', '\\n', texte)\n",
    "    texte = texte.replace('\\nl\\u2019', '\\n')\n",
    "    texte = texte.replace('.', '')\n",
    "\n",
    "    # visual cleaning and tokens agreggation\n",
    "    for item in t_agg:\n",
    "        for t in item[\"rep_texte\"]:\n",
    "            texte = texte.replace(t, item[\"w\"])\n",
    "\n",
    "    # tokenization\n",
    "    texte = texte.replace('\\n\\n', '\\n')\n",
    "    tokens = texte.split(\"\\n\")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte_domaines = texte_domaines1 + \"\\n\" + texte_domaines2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_agg = [ {'w': 'd\\xc3\\xa9v\\xc3\\xa9loppement local\\n', 'rep_texte' :['d\\xe9veloppement local\\n', 'de\\u0301veloppement local\\n', 'decentralisation\\n', 'd\\xe9centralisation\\n', 'd\\xe9concentration administrative\\n', 'd\\xc3\\xa9veloppement local\\n', 'd\\xe9veloppent local\\n', 'd\\xe9senclavement du territoire\\n', 'd\\xe9veloppement local et d\\xe9veloppement local\\n', 'd\\xe9veloppement territoriale et r\\xe9gionale\\n', 'd\\xe9centralisation et d\\xe9veloppement local\\n', 'd\\xc3\\xa9centralisation\\n', 'd\\xc3\\xa9senclavement du territoire\\n', 'd\\xc3\\xa9concentration administrative\\n', 'd\\xc3\\xa9centralisation et d\\xe9veloppement local\\n', 'd\\xc3\\xa9veloppement territoriale et r\\xc3\\xa9gionale\\n', 'de\\xcc\\x81veloppement local\\n']}, {'w': 's\\xc3\\xa9curit\\xc3\\xa9\\n', 'rep_texte' :['s\\xc3\\xa9curit\\xc3\\xa9\\n', 'd\\xe9fense\\n', 's\\xe9curit\\xe9 et d\\xe9fense\\n', 'securite\\n', 'securit\\xe9\\n', 's\\xe9curit\\xe9\\u0301\\n', 's\\xe9curit\\xe9\\u0301\\n', 'se\\u0301curite\\u0301\\n', 's\\xe9curit\\xe9 et s\\xe9curit\\xe9\\n', 'securit\\xc3\\xa9\\n', 's\\xc3\\xa9curit\\xc3\\xa9\\n', 'se\\xcc\\x81curite\\xcc\\x81\\n', 's\\xc3\\xa9curit\\xc3\\xa9\\xcc\\x81\\n', 's\\xc3\\xa9curit\\xc3\\xa9 et d\\xc3\\xa9fense\\n']}, {'w': \"\\xc3\\xa9fficacit\\xc3\\xa9 de l'administration\\n\", 'rep_texte' :['administration g\\xe9n\\xe9rale\\n', 'efficacit\\xe9\\u0301 de l\\u2019administration n\\n', 'efficacit\\xe9\\u0301 de l\\u2019administration\\n', 'efficacite\\u0301 de l\\u2019administration\\n', 'efficacit\\xe9 de l\\u2019administration\\n', \"modernisation de l'administration\\n\", \"administration publique de l'etat\\n\", \"administration publique de l'\\xe9tat\\n\", \"modernisation de l'administration publique\\n\", 'administration g\\xc3\\xa9n\\xc3\\xa9rale\\n', 'efficacit\\xc3\\xa9\\xcc\\x81 de administration n\\n', 'modernisation de administration publique\\n', 'efficacit\\xc3\\xa9 de administration\\n','efficacite\\xcc\\x81 de administration\\n', 'efficacite\\xcc\\x81 de administration\\n', 'efficacit\\xc3\\xa9\\xcc\\x81 de administration\\n', 'administration publique de etat\\n', 'administration publique de \\xc3\\xa9tat\\n', 'modernisation de administration\\n', 'd\\xc3\\xa9crets arr\\xc3\\xaat\\xc3\\xa9s et des nominations\\n']}, {'w': \"transparence\\n\", 'rep_texte' :['direction g\\xc3\\xa9n\\xc3\\xa9rale des imp\\xc3\\xb4ts\\n']}, {'w': 'participation citoyenne\\n', 'rep_texte' :['participation\\n', 'participation citoyenne citoyenne\\n']}, {'w': 'remun\\xc3\\xa9ration du public\\n', 'rep_texte' :['\\xe9quit\\xe9\\u0301\\n', '\\xe9quit\\xe9\\n', 'equit\\xe9\\n', 'r\\xc3\\xa9mun\\xc3\\xa9ration du public\\n', 'r\\xe9mun\\xe9ration du public dans la r\\xe9partition\\n', 'equit\\xe9 dans la r\\xe9partition\\n', 'equite\\u0301\\n', 'e\\u0301quite\\u0301\\n', '\\xc3\\xa9quit\\xc3\\xa9\\n', '\\xc3\\xa9quit\\xc3\\xa9\\xcc\\x81\\n', 'equit\\xc3\\xa9\\n', 'e\\xcc\\x81quite\\xcc\\x81\\n', '\\xc3\\xa9quit\\xc3\\xa9\\xcc\\x81  transparence\\n', '\\xc3\\xa9quit\\xc3\\xa9 dans la r\\xc3\\xa9partition\\n', 's\\xc3\\xa9curit\\xc3\\xa9 sociale\\n']}, {'w': 'corruption\\n', 'rep_texte' :[]}, {'w': 'justice\\n', 'rep_texte' :[]}, {'w': 'sant\\xc3\\xa9\\n', 'rep_texte' :['minist\\xe8re de la sant\\xe9\\n', 'sante\\xcc\\x81\\n', 'sant\\xc3\\xa9\\n', 'sante\\u0301\\n', 'sant\\xe9 humaine et animale\\n', 'minist\\xc3\\xa8re de la sant\\xe9\\n', 'sant\\xc3\\xa9 humaine et animale\\n']}, {'w': '\\xc3\\xa9ducation\\n', 'rep_texte' :['education et enseignement\\n', '\\xc3\\xa9ducation\\n', 'enseignement\\n', 'education\\n']}, {'w': \"acc\\xc3\\xa8s \\xc3\\xa0 l'information\\n\", 'rep_texte' :['information\\n', \"acc\\xc3\\xa8s \\xc3\\xa0 acc\\xc3\\xa8s \\xc3\\xa0 l'information\\n\", \"acc\\xc3\\xa8s a\\xcc\\x80 acc\\xc3\\xa8s \\xc3\\xa0 l'information\\n\", \"acce\\xcc\\x80s a\\xcc\\x80 acc\\xc3\\xa8s \\xc3\\xa0 l'information\\n\"]}, {'w': 'tic\\n', 'rep_texte' :['technologies de l\\u2019information et de la communication\\n', \"technologies de l\\u2019acc\\xc3\\xa8s \\xc3\\xa0 l'information et de la communication\\n\", 'technologies de information et de la communication\\n']}, {'w': 'foncier\\n', 'rep_texte' :['gouvernance fonci\\xe8re\\n', 'foncier rural et immobilier urbain\\n', 'gouvernance fonci\\xc3\\xa8re\\n']}, {'w': 'agriculture\\n', 'rep_texte' :['agriculture et agro business\\n']}, {'w': 'agriculture\\nenvironnement\\n', 'rep_texte' :['agriculture et protection de environnement\\n']}, {'w': 'environnement\\n', 'rep_texte' :[]}, {'w': 'emploi\\n', 'rep_texte' :['travail et emploi\\n', 'entrepreneuriat des jeunes\\n']}, {'w': 'emploi\\nsant\\xc3\\xa9\\n', 'rep_texte' :['durabilit\\xc3\\xa9 dans le domaine de la sant\\xc3\\xa9 et de la jeunesse']}, {'w': 'redevabilit\\xc3\\xa9\\n', 'rep_texte' :['redevabilit\\xe9\\u0301\\n', 'redevabilit\\xc3\\xa9\\xcc\\x81\\n', 'redevabilite\\u0301\\n', 'redevabilit\\xc3\\xa9\\n', 'redevabilite\\xcc\\x81\\n']}, {'w': 'droits humains\\n', 'rep_texte' :[]}, {'w': 'services sociaux de base\\n', 'rep_texte' :[]}, {'w':'transparence\\nredevabilit\\xc3\\xa9\\n', 'rep_texte' :['transparence redevabilit\\xe9\\n']}, {'w':'travail\\njustice\\n', 'rep_texte' :['travail et justice social\\n']}, {'w': \"\\n\\xc3\\xa9fficacit\\xc3\\xa9 de l'administration\\n\",'rep_texte' :['\\nadministration\\n']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte_domaines = clean_and_tokenize(texte_domaines, t_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_domaines = nltk.FreqDist(texte_domaines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_domaines.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
